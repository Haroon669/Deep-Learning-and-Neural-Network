{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"./housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam' , metrics=['mse'] )\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 1ms/step - loss: 285.9977 - mean_squared_error: 285.9977\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 439us/step - loss: 120.6345 - mean_squared_error: 120.6345\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 463us/step - loss: 91.6954 - mean_squared_error: 91.6954\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 497us/step - loss: 75.9991 - mean_squared_error: 75.9991\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 389us/step - loss: 72.4581 - mean_squared_error: 72.4581\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 384us/step - loss: 70.4711 - mean_squared_error: 70.4711\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 372us/step - loss: 67.7964 - mean_squared_error: 67.7964\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 392us/step - loss: 67.4665 - mean_squared_error: 67.4665\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 410us/step - loss: 64.0928 - mean_squared_error: 64.0928\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 388us/step - loss: 61.1500 - mean_squared_error: 61.1500\n",
      "51/51 [==============================] - 0s 1ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 1ms/step - loss: 274.2083 - mean_squared_error: 274.2083\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 481us/step - loss: 100.6609 - mean_squared_error: 100.66090s - loss: 114.1402 - mean_squared_error: \n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 462us/step - loss: 85.6075 - mean_squared_error: 85.6075\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 410us/step - loss: 72.7170 - mean_squared_error: 72.7170\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 396us/step - loss: 67.1845 - mean_squared_error: 67.1845\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 391us/step - loss: 63.4239 - mean_squared_error: 63.4239\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 378us/step - loss: 59.9254 - mean_squared_error: 59.9254\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 429us/step - loss: 60.1630 - mean_squared_error: 60.1630\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 409us/step - loss: 56.6785 - mean_squared_error: 56.6785\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 411us/step - loss: 55.9783 - mean_squared_error: 55.9783\n",
      "51/51 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 324.7358 - mean_squared_error: 324.7358\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 442us/step - loss: 105.4849 - mean_squared_error: 105.4849\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 407us/step - loss: 84.9718 - mean_squared_error: 84.9718\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 398us/step - loss: 77.9229 - mean_squared_error: 77.9229\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 407us/step - loss: 75.3939 - mean_squared_error: 75.3939\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 431us/step - loss: 75.7699 - mean_squared_error: 75.7699\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 389us/step - loss: 72.6294 - mean_squared_error: 72.6294\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 354us/step - loss: 71.6668 - mean_squared_error: 71.6668\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 367us/step - loss: 72.5759 - mean_squared_error: 72.5759\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 356us/step - loss: 68.7403 - mean_squared_error: 68.7403\n",
      "51/51 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 1ms/step - loss: 410.7030 - mean_squared_error: 410.7030\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 451us/step - loss: 111.2816 - mean_squared_error: 111.2816\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 385us/step - loss: 64.4699 - mean_squared_error: 64.4699\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 365us/step - loss: 56.8002 - mean_squared_error: 56.8002\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 347us/step - loss: 54.4378 - mean_squared_error: 54.4378\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 332us/step - loss: 51.2491 - mean_squared_error: 51.2491\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 328us/step - loss: 50.9848 - mean_squared_error: 50.9848\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 358us/step - loss: 48.1928 - mean_squared_error: 48.1928\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 328us/step - loss: 48.1683 - mean_squared_error: 48.1683\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 323us/step - loss: 45.9211 - mean_squared_error: 45.9211\n",
      "51/51 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 312.0760 - mean_squared_error: 312.0760\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 519us/step - loss: 102.6392 - mean_squared_error: 102.6392\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 420us/step - loss: 74.3683 - mean_squared_error: 74.3683\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 416us/step - loss: 69.1140 - mean_squared_error: 69.1140\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 411us/step - loss: 67.3695 - mean_squared_error: 67.3695\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 431us/step - loss: 65.4967 - mean_squared_error: 65.4967\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 400us/step - loss: 63.5909 - mean_squared_error: 63.5909\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 339us/step - loss: 60.1728 - mean_squared_error: 60.1728\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 323us/step - loss: 58.6372 - mean_squared_error: 58.6372\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 325us/step - loss: 55.6444 - mean_squared_error: 55.6444\n",
      "51/51 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 142.1424 - mean_squared_error: 142.1424\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 418us/step - loss: 80.5437 - mean_squared_error: 80.5437\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 433us/step - loss: 67.8910 - mean_squared_error: 67.8910\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 446us/step - loss: 64.1584 - mean_squared_error: 64.1584\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 431us/step - loss: 61.4832 - mean_squared_error: 61.4832\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 422us/step - loss: 58.5431 - mean_squared_error: 58.5431\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 402us/step - loss: 54.4243 - mean_squared_error: 54.4243\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 405us/step - loss: 53.3510 - mean_squared_error: 53.3510\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 418us/step - loss: 52.1150 - mean_squared_error: 52.1150\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 422us/step - loss: 52.0786 - mean_squared_error: 52.0786\n",
      "51/51 [==============================] - 0s 4ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 1s 2ms/step - loss: 201.0363 - mean_squared_error: 201.0363\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 522us/step - loss: 102.5336 - mean_squared_error: 102.5336\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 466us/step - loss: 85.0551 - mean_squared_error: 85.0551\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 505us/step - loss: 78.5900 - mean_squared_error: 78.5900\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 468us/step - loss: 77.4019 - mean_squared_error: 77.4019\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 522us/step - loss: 71.6509 - mean_squared_error: 71.6509\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 480us/step - loss: 69.5955 - mean_squared_error: 69.5955\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 507us/step - loss: 66.6637 - mean_squared_error: 66.6637\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 0s 448us/step - loss: 63.9087 - mean_squared_error: 63.9087\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 456us/step - loss: 61.3848 - mean_squared_error: 61.3848\n",
      "50/50 [==============================] - 0s 4ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 1s 2ms/step - loss: 203.9889 - mean_squared_error: 203.9889\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 399us/step - loss: 80.8669 - mean_squared_error: 80.8669\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 360us/step - loss: 63.2224 - mean_squared_error: 63.2224\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 454us/step - loss: 59.1312 - mean_squared_error: 59.1312\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 424us/step - loss: 57.2919 - mean_squared_error: 57.2919\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 474us/step - loss: 53.9805 - mean_squared_error: 53.9805\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 414us/step - loss: 54.1456 - mean_squared_error: 54.1456\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 410us/step - loss: 51.5274 - mean_squared_error: 51.5274\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 478us/step - loss: 50.5394 - mean_squared_error: 50.5394\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 428us/step - loss: 50.0388 - mean_squared_error: 50.0388\n",
      "50/50 [==============================] - 0s 4ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 1s 2ms/step - loss: 236.0009 - mean_squared_error: 236.0009\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 564us/step - loss: 102.6191 - mean_squared_error: 102.6191\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 535us/step - loss: 83.4581 - mean_squared_error: 83.4581\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 487us/step - loss: 71.3254 - mean_squared_error: 71.3254\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 509us/step - loss: 65.2854 - mean_squared_error: 65.2854\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 537us/step - loss: 63.6784 - mean_squared_error: 63.6784\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 524us/step - loss: 61.0671 - mean_squared_error: 61.0671\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 514us/step - loss: 59.0582 - mean_squared_error: 59.0582\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 487us/step - loss: 58.0002 - mean_squared_error: 58.0002\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 529us/step - loss: 55.3283 - mean_squared_error: 55.3283\n",
      "50/50 [==============================] - 0s 5ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 1s 2ms/step - loss: 264.5214 - mean_squared_error: 264.5214\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 515us/step - loss: 112.6603 - mean_squared_error: 112.6603\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 516us/step - loss: 83.0528 - mean_squared_error: 83.0528\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 498us/step - loss: 75.4779 - mean_squared_error: 75.4779\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 555us/step - loss: 73.7023 - mean_squared_error: 73.7023\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 433us/step - loss: 72.2964 - mean_squared_error: 72.2964\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 430us/step - loss: 70.8063 - mean_squared_error: 70.8063\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 404us/step - loss: 68.9119 - mean_squared_error: 68.9119\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 413us/step - loss: 69.2395 - mean_squared_error: 69.2395\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 469us/step - loss: 67.7312 - mean_squared_error: 67.7312\n",
      "50/50 [==============================] - 0s 5ms/step\n",
      "Results: -57.78% (42.28%) MSE\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 598.4578 - mean_squared_error: 598.4578\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 532us/step - loss: 553.6127 - mean_squared_error: 553.6127\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 527us/step - loss: 461.4024 - mean_squared_error: 461.4024\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 635us/step - loss: 337.8725 - mean_squared_error: 337.8725\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 543us/step - loss: 224.0726 - mean_squared_error: 224.0726\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 499us/step - loss: 142.4471 - mean_squared_error: 142.4471\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 503us/step - loss: 95.4888 - mean_squared_error: 95.4888\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 490us/step - loss: 70.3585 - mean_squared_error: 70.3585\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 435us/step - loss: 55.4823 - mean_squared_error: 55.4823\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 421us/step - loss: 46.0124 - mean_squared_error: 46.0124\n",
      "51/51 [==============================] - 0s 5ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 583.7384 - mean_squared_error: 583.7384\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 526us/step - loss: 549.6146 - mean_squared_error: 549.6146\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 522us/step - loss: 478.2904 - mean_squared_error: 478.2904\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 574us/step - loss: 380.8298 - mean_squared_error: 380.8298\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 497us/step - loss: 279.5786 - mean_squared_error: 279.5786\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 468us/step - loss: 195.1115 - mean_squared_error: 195.1115\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 503us/step - loss: 136.1547 - mean_squared_error: 136.1547\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 446us/step - loss: 99.2328 - mean_squared_error: 99.2328\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 497us/step - loss: 76.9392 - mean_squared_error: 76.9392\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 440us/step - loss: 62.7738 - mean_squared_error: 62.7738\n",
      "51/51 [==============================] - 0s 5ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 614.0248 - mean_squared_error: 614.0248\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 438us/step - loss: 577.9363 - mean_squared_error: 577.9363\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 446us/step - loss: 501.6268 - mean_squared_error: 501.6268\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 473us/step - loss: 391.2735 - mean_squared_error: 391.2735\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 521us/step - loss: 278.4133 - mean_squared_error: 278.4133\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 429us/step - loss: 186.6185 - mean_squared_error: 186.6185\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 391us/step - loss: 122.5787 - mean_squared_error: 122.5787\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 370us/step - loss: 82.9863 - mean_squared_error: 82.9863\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 372us/step - loss: 59.8944 - mean_squared_error: 59.8944\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 413us/step - loss: 47.2475 - mean_squared_error: 47.2475\n",
      "51/51 [==============================] - 0s 5ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 532.6644 - mean_squared_error: 532.6644\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 379us/step - loss: 506.2724 - mean_squared_error: 506.2724\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 376us/step - loss: 445.4080 - mean_squared_error: 445.4080\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 374us/step - loss: 344.8283 - mean_squared_error: 344.8283\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 363us/step - loss: 236.9364 - mean_squared_error: 236.9364\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 402us/step - loss: 152.8314 - mean_squared_error: 152.8314\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 371us/step - loss: 99.8638 - mean_squared_error: 99.8638\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 356us/step - loss: 71.2346 - mean_squared_error: 71.2346\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 358us/step - loss: 55.6528 - mean_squared_error: 55.6528\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 358us/step - loss: 46.1730 - mean_squared_error: 46.1730\n",
      "51/51 [==============================] - 0s 5ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 3ms/step - loss: 557.4974 - mean_squared_error: 557.4974\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 558us/step - loss: 519.4783 - mean_squared_error: 519.4783\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 561us/step - loss: 428.7952 - mean_squared_error: 428.7952\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 492us/step - loss: 310.8088 - mean_squared_error: 310.8088\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 499us/step - loss: 202.8941 - mean_squared_error: 202.8941\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 477us/step - loss: 128.0753 - mean_squared_error: 128.0753\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 517us/step - loss: 86.0895 - mean_squared_error: 86.0895\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 457us/step - loss: 64.0337 - mean_squared_error: 64.0337\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 427us/step - loss: 51.0943 - mean_squared_error: 51.0943\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 460us/step - loss: 42.8698 - mean_squared_error: 42.8698\n",
      "51/51 [==============================] - 0s 6ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 532.7206 - mean_squared_error: 532.7206\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 446us/step - loss: 494.8183 - mean_squared_error: 494.8183\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 468us/step - loss: 412.1762 - mean_squared_error: 412.1762\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 490us/step - loss: 307.0197 - mean_squared_error: 307.0197\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 468us/step - loss: 209.1563 - mean_squared_error: 209.1563\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 473us/step - loss: 137.7036 - mean_squared_error: 137.7036\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 473us/step - loss: 93.8469 - mean_squared_error: 93.8469\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 510us/step - loss: 69.2237 - mean_squared_error: 69.2237\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 475us/step - loss: 54.8678 - mean_squared_error: 54.8678\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 492us/step - loss: 45.3669 - mean_squared_error: 45.3669\n",
      "51/51 [==============================] - 0s 7ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 1s 3ms/step - loss: 593.6614 - mean_squared_error: 593.6614\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 579us/step - loss: 546.7467 - mean_squared_error: 546.7467\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 608us/step - loss: 449.7920 - mean_squared_error: 449.7920\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 537us/step - loss: 316.6414 - mean_squared_error: 316.6414\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 573us/step - loss: 198.8420 - mean_squared_error: 198.8420\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 568us/step - loss: 125.7289 - mean_squared_error: 125.7289\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 544us/step - loss: 87.1073 - mean_squared_error: 87.1073\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 544us/step - loss: 66.2473 - mean_squared_error: 66.2473\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 0s 598us/step - loss: 53.5777 - mean_squared_error: 53.5777\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 533us/step - loss: 45.4077 - mean_squared_error: 45.4077\n",
      "50/50 [==============================] - 0s 8ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 1s 3ms/step - loss: 596.8758 - mean_squared_error: 596.8758\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 541us/step - loss: 545.3733 - mean_squared_error: 545.3733\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 552us/step - loss: 435.5485 - mean_squared_error: 435.5485\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 526us/step - loss: 306.4228 - mean_squared_error: 306.4228\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 527us/step - loss: 202.7402 - mean_squared_error: 202.7402\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 510us/step - loss: 134.3020 - mean_squared_error: 134.3020\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 572us/step - loss: 91.2056 - mean_squared_error: 91.2056\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 567us/step - loss: 64.8924 - mean_squared_error: 64.8924\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 647us/step - loss: 48.6968 - mean_squared_error: 48.6968\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 579us/step - loss: 39.1092 - mean_squared_error: 39.1092\n",
      "50/50 [==============================] - 0s 8ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 2s 4ms/step - loss: 623.3570 - mean_squared_error: 623.3570\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 446us/step - loss: 569.1051 - mean_squared_error: 569.1051\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 387us/step - loss: 465.9764 - mean_squared_error: 465.9764\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 376us/step - loss: 348.8322 - mean_squared_error: 348.8322\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 384us/step - loss: 241.6747 - mean_squared_error: 241.6747\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 443us/step - loss: 164.9747 - mean_squared_error: 164.9747\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 388us/step - loss: 117.2247 - mean_squared_error: 117.2247\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 393us/step - loss: 87.4162 - mean_squared_error: 87.4162\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 397us/step - loss: 68.3184 - mean_squared_error: 68.3184\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 378us/step - loss: 56.4722 - mean_squared_error: 56.4722\n",
      "50/50 [==============================] - 0s 10ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 1s 3ms/step - loss: 609.6327 - mean_squared_error: 609.6327\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 537us/step - loss: 572.0412 - mean_squared_error: 572.0412\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 553us/step - loss: 476.6110 - mean_squared_error: 476.6110\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 652us/step - loss: 347.0990 - mean_squared_error: 347.0990\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 575us/step - loss: 228.7372 - mean_squared_error: 228.7372\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 483us/step - loss: 147.8019 - mean_squared_error: 147.8019\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 513us/step - loss: 99.5731 - mean_squared_error: 99.5731\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 529us/step - loss: 71.0047 - mean_squared_error: 71.0047\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 502us/step - loss: 54.3838 - mean_squared_error: 54.3838\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 476us/step - loss: 43.9167 - mean_squared_error: 43.9167\n",
      "50/50 [==============================] - 0s 9ms/step\n",
      "Standardized: -58.84 (46.35) MSE\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 3ms/step - loss: 582.8514 - mean_squared_error: 582.8514\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 589us/step - loss: 574.7994 - mean_squared_error: 574.7994\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 631us/step - loss: 567.7357 - mean_squared_error: 567.7357\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 597us/step - loss: 564.8679 - mean_squared_error: 564.8679\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 651us/step - loss: 563.9618 - mean_squared_error: 563.9618\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 673us/step - loss: 563.5980 - mean_squared_error: 563.5980\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 477us/step - loss: 563.4150 - mean_squared_error: 563.4150\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 475us/step - loss: 563.3105 - mean_squared_error: 563.3105\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 505us/step - loss: 563.2448 - mean_squared_error: 563.2448\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 536us/step - loss: 563.2006 - mean_squared_error: 563.2006\n",
      "51/51 [==============================] - 0s 8ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 3ms/step - loss: 565.8134 - mean_squared_error: 565.8134\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 592us/step - loss: 558.9724 - mean_squared_error: 558.9724\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 633us/step - loss: 551.9298 - mean_squared_error: 551.9298\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 633us/step - loss: 548.3394 - mean_squared_error: 548.3394\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 578us/step - loss: 546.9313 - mean_squared_error: 546.9313\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 583us/step - loss: 546.3542 - mean_squared_error: 546.3542\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 631us/step - loss: 546.0755 - mean_squared_error: 546.0755\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 616us/step - loss: 545.9231 - mean_squared_error: 545.9231\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 635us/step - loss: 545.8307 - mean_squared_error: 545.8307\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 679us/step - loss: 545.7706 - mean_squared_error: 545.7706\n",
      "51/51 [==============================] - 1s 14ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 2s 4ms/step - loss: 596.5658 - mean_squared_error: 596.5658\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 530us/step - loss: 589.3367 - mean_squared_error: 589.3367\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 485us/step - loss: 582.1156 - mean_squared_error: 582.1156\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 481us/step - loss: 578.4105 - mean_squared_error: 578.4105\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 497us/step - loss: 577.1379 - mean_squared_error: 577.1379\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 558us/step - loss: 576.6546 - mean_squared_error: 576.6546\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 501us/step - loss: 576.4274 - mean_squared_error: 576.4274\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 499us/step - loss: 576.3019 - mean_squared_error: 576.3019\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 499us/step - loss: 576.2257 - mean_squared_error: 576.2257\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 530us/step - loss: 576.1760 - mean_squared_error: 576.1760\n",
      "51/51 [==============================] - 0s 10ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 3ms/step - loss: 515.5362 - mean_squared_error: 515.5362\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 646us/step - loss: 509.6983 - mean_squared_error: 509.6983\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 622us/step - loss: 502.3562 - mean_squared_error: 502.3562\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 550us/step - loss: 498.6001 - mean_squared_error: 498.6001\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 541us/step - loss: 497.2447 - mean_squared_error: 497.2447\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 594us/step - loss: 496.7206 - mean_squared_error: 496.7206\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 479us/step - loss: 496.4793 - mean_squared_error: 496.4793\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 415us/step - loss: 496.3514 - mean_squared_error: 496.3514\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 407us/step - loss: 496.2758 - mean_squared_error: 496.2758\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 400us/step - loss: 496.2274 - mean_squared_error: 496.2274\n",
      "51/51 [==============================] - 1s 10ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 2s 4ms/step - loss: 541.0079 - mean_squared_error: 541.0079\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 629us/step - loss: 533.4913 - mean_squared_error: 533.4913\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 644us/step - loss: 526.0512 - mean_squared_error: 526.0512\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 631us/step - loss: 523.3639 - mean_squared_error: 523.3639\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 624us/step - loss: 522.4565 - mean_squared_error: 522.4565\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 662us/step - loss: 522.0735 - mean_squared_error: 522.0735\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 563us/step - loss: 521.8789 - mean_squared_error: 521.8789\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 536us/step - loss: 521.7676 - mean_squared_error: 521.7676\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 550us/step - loss: 521.6978 - mean_squared_error: 521.6978\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 550us/step - loss: 521.6512 - mean_squared_error: 521.6512\n",
      "51/51 [==============================] - 1s 11ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 2s 4ms/step - loss: 516.5915 - mean_squared_error: 516.5915\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 605us/step - loss: 509.0812 - mean_squared_error: 509.0812\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 642us/step - loss: 501.9991 - mean_squared_error: 501.9991\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 543us/step - loss: 499.4041 - mean_squared_error: 499.4041\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 521us/step - loss: 498.4582 - mean_squared_error: 498.4582\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 521us/step - loss: 498.0505 - mean_squared_error: 498.0505\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 558us/step - loss: 497.8463 - mean_squared_error: 497.8463\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 440us/step - loss: 497.7309 - mean_squared_error: 497.7309\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 409us/step - loss: 497.6591 - mean_squared_error: 497.6591\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 418us/step - loss: 497.6117 - mean_squared_error: 497.6117\n",
      "51/51 [==============================] - 0s 9ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 2s 4ms/step - loss: 578.3685 - mean_squared_error: 578.3685\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 568us/step - loss: 569.6486 - mean_squared_error: 569.6486\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 625us/step - loss: 562.9193 - mean_squared_error: 562.9193\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 601us/step - loss: 560.5002 - mean_squared_error: 560.5002\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 610us/step - loss: 559.6590 - mean_squared_error: 559.6590\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 603us/step - loss: 559.3011 - mean_squared_error: 559.3011\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 634us/step - loss: 559.1199 - mean_squared_error: 559.1199\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 0s 581us/step - loss: 559.0164 - mean_squared_error: 559.0164\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 554us/step - loss: 558.9517 - mean_squared_error: 558.9517\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 616us/step - loss: 558.9088 - mean_squared_error: 558.9088\n",
      "50/50 [==============================] - 1s 11ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 2s 4ms/step - loss: 582.0384 - mean_squared_error: 582.0384\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 626us/step - loss: 573.5877 - mean_squared_error: 573.5877\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 590us/step - loss: 567.0611 - mean_squared_error: 567.0611\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 571us/step - loss: 564.3746 - mean_squared_error: 564.3746\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 654us/step - loss: 563.3254 - mean_squared_error: 563.3254\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 651us/step - loss: 562.8770 - mean_squared_error: 562.8770\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 587us/step - loss: 562.6526 - mean_squared_error: 562.6526\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 657us/step - loss: 562.5239 - mean_squared_error: 562.5239\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 611us/step - loss: 562.4425 - mean_squared_error: 562.4425\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 507us/step - loss: 562.3896 - mean_squared_error: 562.3896\n",
      "50/50 [==============================] - 1s 14ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 2s 4ms/step - loss: 608.7206 - mean_squared_error: 608.7206\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 485us/step - loss: 600.1749 - mean_squared_error: 600.1749\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 623us/step - loss: 594.3290 - mean_squared_error: 594.3290\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 638us/step - loss: 591.4651 - mean_squared_error: 591.4651\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 594us/step - loss: 589.9846 - mean_squared_error: 589.9846\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 597us/step - loss: 589.3442 - mean_squared_error: 589.3442\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 642us/step - loss: 589.0833 - mean_squared_error: 589.0833\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 612us/step - loss: 588.9513 - mean_squared_error: 588.9513\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 605us/step - loss: 588.8744 - mean_squared_error: 588.8744\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 645us/step - loss: 588.8252 - mean_squared_error: 588.8252\n",
      "50/50 [==============================] - 1s 13ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 2s 4ms/step - loss: 591.9043 - mean_squared_error: 591.9043\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 619us/step - loss: 584.2827 - mean_squared_error: 584.2827\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 598us/step - loss: 576.0795 - mean_squared_error: 576.0795\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 669us/step - loss: 573.1293 - mean_squared_error: 573.1293\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 530us/step - loss: 572.1971 - mean_squared_error: 572.1971\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 511us/step - loss: 571.8312 - mean_squared_error: 571.8312\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 502us/step - loss: 571.6562 - mean_squared_error: 571.6562\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 555us/step - loss: 571.5591 - mean_squared_error: 571.5591\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 532us/step - loss: 571.4998 - mean_squared_error: 571.4998\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 581us/step - loss: 571.4608 - mean_squared_error: 571.4608\n",
      "50/50 [==============================] - 1s 14ms/step\n",
      "Standardized: -546.70 (276.40) MSE\n"
     ]
    }
   ],
   "source": [
    "#using diffrent activation function on output layer\n",
    "\n",
    "# define base model\n",
    "def with_some_change_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal',activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam' , metrics=['mse'] )\n",
    "\treturn model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=with_some_change_model, epochs=10, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: -42.68% (46.50%) MSE\n"
     ]
    }
   ],
   "source": [
    "#using diffrent activation function on output layer\n",
    "\n",
    "# define base model\n",
    "def new_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(6,  kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam' )\n",
    "    return model\n",
    "# evaluate model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=new_model, epochs=10, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wider: -37.36% (35.31%) MSE\n"
     ]
    }
   ],
   "source": [
    "#using Wider Network\n",
    "\n",
    "# define base model\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(6,  kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam' )\n",
    "    return model\n",
    "# evaluate model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=10, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Wider: %.2f%% (%.2f%%) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wider: -37.38% (35.04%) MSE\n"
     ]
    }
   ],
   "source": [
    "#using diffrent activation function on output layer\n",
    "\n",
    "# define base model\n",
    "def overfit_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(13,  kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(10,  kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    model.add(Dense(6,  kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam' )\n",
    "    return model\n",
    "# evaluate model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=10, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Wider: %.2f%% (%.2f%%) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "455/455 [==============================] - 4s 9ms/step - loss: 598.4631\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 941us/step - loss: 553.7137\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 942us/step - loss: 461.5910\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 952us/step - loss: 338.0172\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 879us/step - loss: 224.0729\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 972us/step - loss: 142.4745\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 970us/step - loss: 95.5640\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 1s 1ms/step - loss: 70.4599\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 55.5819\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 860us/step - loss: 46.1016\n",
      "51/51 [==============================] - 1s 28ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 4s 10ms/step - loss: 583.7432\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 752us/step - loss: 549.6669\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 837us/step - loss: 478.4014\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 817us/step - loss: 380.9406\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 766us/step - loss: 279.6790\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 195.1871\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 889us/step - loss: 136.2008\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 980us/step - loss: 99.2611\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 979us/step - loss: 76.9574\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 62.7866\n",
      "51/51 [==============================] - 2s 34ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 4s 9ms/step - loss: 613.9955\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 985us/step - loss: 577.8593\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 501.4586\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 945us/step - loss: 391.0473\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 278.2365\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 902us/step - loss: 186.5280\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 1s 1ms/step - loss: 121.6690\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 80.9289\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 792us/step - loss: 58.3031\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 873us/step - loss: 46.2209\n",
      "51/51 [==============================] - 1s 26ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 4s 9ms/step - loss: 532.6661\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 939us/step - loss: 506.2940\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 445.4782\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 927us/step - loss: 344.9145\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 236.9505\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 962us/step - loss: 152.7400\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 993us/step - loss: 99.7037\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 948us/step - loss: 71.0154\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 948us/step - loss: 55.4335\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 45.9952\n",
      "51/51 [==============================] - 2s 30ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 4s 9ms/step - loss: 557.4944\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 621us/step - loss: 519.5376\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 816us/step - loss: 429.0020\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 311.0106\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 921us/step - loss: 203.0786\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 128.0591\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 945us/step - loss: 85.9070\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 63.7946\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 813us/step - loss: 50.8366\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 0s 787us/step - loss: 42.6250\n",
      "51/51 [==============================] - 2s 32ms/step\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 4s 10ms/step - loss: 532.8233\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 0s 892us/step - loss: 495.2616\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 412.6110\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 0s 912us/step - loss: 307.2384\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 0s 969us/step - loss: 209.1298\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 0s 764us/step - loss: 137.5098\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 0s 782us/step - loss: 93.6004\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 0s 1ms/step - loss: 68.9554\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 0s 948us/step - loss: 54.0798\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 1s 1ms/step - loss: 44.2005\n",
      "51/51 [==============================] - 2s 34ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 4s 8ms/step - loss: 593.6065\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 826us/step - loss: 546.6225\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 858us/step - loss: 452.6876\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 792us/step - loss: 325.3541\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 838us/step - loss: 206.5462\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 853us/step - loss: 128.5494\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 803us/step - loss: 86.8027\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 814us/step - loss: 65.5986\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 860us/step - loss: 53.3701\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 814us/step - loss: 45.5064\n",
      "50/50 [==============================] - 2s 34ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 4s 10ms/step - loss: 596.8850\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 1ms/step - loss: 545.4062\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 823us/step - loss: 435.6178\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 739us/step - loss: 306.4920\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 671us/step - loss: 202.7791\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 646us/step - loss: 134.3261\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 618us/step - loss: 91.2166\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 884us/step - loss: 64.8986\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 1ms/step - loss: 48.7006\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 948us/step - loss: 39.1114\n",
      "50/50 [==============================] - 2s 32ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 4s 8ms/step - loss: 623.3045\n",
      "Epoch 2/10\n",
      "456/456 [==============================] - 0s 1ms/step - loss: 569.0066\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 983us/step - loss: 466.1703\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 1ms/step - loss: 349.2742\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 932us/step - loss: 243.2409\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 1ms/step - loss: 167.4206\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 939us/step - loss: 118.7117\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 930us/step - loss: 87.2701\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 650us/step - loss: 66.9726\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 636us/step - loss: 54.5188\n",
      "50/50 [==============================] - 2s 31ms/step\n",
      "Epoch 1/10\n",
      "456/456 [==============================] - 4s 8ms/step - loss: 609.5612\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 0s 902us/step - loss: 571.7822\n",
      "Epoch 3/10\n",
      "456/456 [==============================] - 0s 970us/step - loss: 475.9967\n",
      "Epoch 4/10\n",
      "456/456 [==============================] - 0s 907us/step - loss: 346.6807\n",
      "Epoch 5/10\n",
      "456/456 [==============================] - 0s 886us/step - loss: 228.5868\n",
      "Epoch 6/10\n",
      "456/456 [==============================] - 0s 1ms/step - loss: 147.8408\n",
      "Epoch 7/10\n",
      "456/456 [==============================] - 0s 903us/step - loss: 99.6997\n",
      "Epoch 8/10\n",
      "456/456 [==============================] - 0s 970us/step - loss: 71.1359\n",
      "Epoch 9/10\n",
      "456/456 [==============================] - 0s 852us/step - loss: 54.4886\n",
      "Epoch 10/10\n",
      "456/456 [==============================] - 0s 927us/step - loss: 43.9935\n",
      "50/50 [==============================] - 2s 30ms/step\n",
      "Wider: -57.69% (46.43%) MSE\n"
     ]
    }
   ],
   "source": [
    "# Tunnung Model for accuracy\n",
    "\n",
    "# define base model\n",
    "def accuracy_of_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam' )\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=accuracy_of_model, epochs=10, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Wider: %.2f%% (%.2f%%) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 50894.50% (111761.81%)\n"
     ]
    }
   ],
   "source": [
    "#Rewriting using functional Api model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# This returns a tensor\n",
    "inputs = Input(shape=(13,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(13, activation='relu')(inputs)\n",
    "predictions = Dense(1)(x)\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and two Dense layers\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'] )\n",
    "history=model.fit(X,Y,epochs=10, batch_size=5, verbose=False)\n",
    "# print(model.summary())\n",
    "\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "acc_values = history_dict['mean_squared_error']\n",
    "print(\"Result: %.2f%% (%.2f%%)\" % (numpy.mean(acc_values)*100, numpy.std(acc_values)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport tensorflow as tf\\nimport keras\\nclass MyModel(tf.keras.Model):\\n    \\n        def __init__(self):\\n            super(MyModel, self).__init__()\\n            self.dense1= Dense(13, activation=\\'relu\\')\\n            self.dense2 = Dense(1)\\n\\n        def call(self, inputs):\\n            x = self.dense1(inputs)\\n            return self.dense2(x)\\n                        \\n                    \\nmodel = MyModel()\\nmodel.compile(loss=\\'mean_squared_error\\', optimizer=\\'adam\\',metrics=[\\'mean_squared_error\\'])\\nhistory=model.fit(X,Y, epochs=10,batch_size=5,verbose=False)\\nhistory_dict = history.history\\nhistory_dict.keys()\\nacc_values = history_dict[\\'mean_squared_error\\']\\nprint(\"Result: %.2f%% (%.2f%%)\" % (numpy.mean(acc_values)*100, numpy.std(acc_values)*100))\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "        def __init__(self):\n",
    "            super(MyModel, self).__init__()\n",
    "            self.dense1= Dense(13, activation='relu')\n",
    "            self.dense2 = Dense(1)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            x = self.dense1(inputs)\n",
    "            return self.dense2(x)\n",
    "                        \n",
    "                    \n",
    "model = MyModel()\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['mean_squared_error'])\n",
    "history=model.fit(X,Y, epochs=10,batch_size=5,verbose=False)\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "acc_values = history_dict['mean_squared_error']\n",
    "print(\"Result: %.2f%% (%.2f%%)\" % (numpy.mean(acc_values)*100, numpy.std(acc_values)*100))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n",
      "processing fold # 4\n",
      "processing fold # 5\n",
      "processing fold # 6\n",
      "processing fold # 7\n",
      "processing fold # 8\n",
      "processing fold # 9\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "def build_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal' ,activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "k=10\n",
    "num_val_samples = len(X) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    val_data = X[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = Y[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "#     print(val_data,val_targets)\n",
    "    partial_train_data =numpy.concatenate([X[:i * num_val_samples],X[(i + 1) * num_val_samples:]],axis=0)\n",
    "    partial_train_targets =numpy.concatenate([Y[:i * num_val_samples],Y[(i + 1) * num_val_samples:]],axis=0)\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=5, verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 1.40% (1.56%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Result: %.2f%% (%.2f%%)\" % (numpy.mean(all_scores)*100, numpy.std(all_scores)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
